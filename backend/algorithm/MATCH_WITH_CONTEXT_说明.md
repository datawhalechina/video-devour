# 文本块匹配优化说明

## 概述
本次更新优化了文本块与大纲标题的匹配功能，在匹配过程中增加了**原文上下文参考**，显著提高了匹配的准确度。

## 改进内容

### 1. 增强的匹配方法 (`llm_handler.py`)

#### 新增参数
`match_chunk_to_headings()` 方法新增了以下参数：
- `chunk_index` (int, optional): 当前文本块在完整对话列表中的索引位置
- `all_chunks` (list, optional): 完整的对话文本块列表，用于提供上下文
- `context_window` (int): 上下文窗口大小，默认为 3（即前后各 3 个文本块）

#### 功能增强
1. **自动提取上下文**：当提供 `chunk_index` 和 `all_chunks` 参数时，系统会自动提取当前文本块前后各 N 个文本块作为上下文
2. **可视化标记**：在 prompt 中用 `>>> ... <<<` 清晰标记当前待匹配的文本块
3. **完整时间戳**：上下文中的所有文本块都保留原始的时间戳和说话人信息
4. **向后兼容**：如果不提供新参数，方法会回退到原来的简单匹配模式

### 2. 调用方式更新 (`main.py`)

在主流程中，调用匹配方法时传入完整的对话列表：

```python
matched_heading = llm.match_chunk_to_headings(
    chunk, 
    candidate_headings, 
    chunk_index=i,           # 当前块的索引
    all_chunks=processed_dialogue  # 完整对话列表
)
```

## 工作原理

### 示例场景

假设有以下对话序列（简化版）：

```
[00:00 - 00:15] SPEAKER_0: 今天我们讲讲冰墩墩的故事
[00:15 - 00:30] SPEAKER_0: 冰字象征纯洁、坚强
[00:30 - 00:45] SPEAKER_0: 墩墩意味着敦厚可爱        <- 当前待匹配块
[00:45 - 01:00] SPEAKER_0: 这个名字有深刻含义
[01:00 - 01:15] SPEAKER_1: 那我们来介绍自己的名字
```

当匹配中间的文本块（"墩墩意味着敦厚可爱"）时，LLM 会看到：

```
**原文参考（带上下文）:**
[00:00 - 00:15] SPEAKER_0: 今天我们讲讲冰墩墩的故事
[00:15 - 00:30] SPEAKER_0: 冰字象征纯洁、坚强
>>> [00:30 - 00:45] SPEAKER_0: 墩墩意味着敦厚可爱 <<<  [当前待匹配文本块]
[00:45 - 01:00] SPEAKER_0: 这个名字有深刻含义
[01:00 - 01:15] SPEAKER_1: 那我们来介绍自己的名字
```

这样 LLM 可以：
- 理解当前块在整体话题中的位置
- 判断是否处于话题切换点
- 基于更完整的语义做出更准确的标题匹配决策

## 优势

1. **提高匹配准确度**：通过上下文信息，LLM 能更准确地理解文本块的主题归属
2. **避免误判**：减少因单个文本块信息不足导致的错误分类
3. **处理话题过渡**：更好地识别话题切换点
4. **保持时间连贯性**：确保相邻文本块在语义和时间上的连贯性

## 配置选项

### 调整上下文窗口大小

如果需要调整上下文的大小，可以在调用时指定 `context_window` 参数：

```python
matched_heading = llm.match_chunk_to_headings(
    chunk, 
    candidate_headings, 
    chunk_index=i,
    all_chunks=processed_dialogue,
    context_window=5  # 增加到前后各 5 个块
)
```

**建议配置**：
- 对于简短对话：`context_window=2` 或 `3`（默认）
- 对于长对话或复杂主题：`context_window=5` 或更大
- 注意：窗口过大会增加 token 消耗和 API 调用成本

## 日志输出

系统会在日志中记录上下文使用情况：

```
为匹配任务添加了上下文参考（前 3 个块，后 3 个块）
```

这有助于调试和监控匹配过程。

## 向后兼容性

如果不提供 `chunk_index` 和 `all_chunks` 参数，方法会自动回退到原来的匹配模式（无上下文），确保与旧代码的兼容性。

## 性能考虑

- **Token 消耗**：添加上下文会增加每次匹配的 token 消耗（约增加 6-10 倍，取决于上下文窗口）
- **匹配质量**：但匹配准确度的提升通常能减少后续人工修正的成本
- **建议**：对于关键项目，这个 trade-off 是值得的

## 相关文件

- `backend/algorithm/llm_handler.py` - 核心匹配逻辑
- `backend/algorithm/main.py` - 主流程调用
- `backend/algorithm/data_processor.py` - 数据预处理

## 后续优化方向

1. **智能上下文选择**：根据文本块的语义相似度动态选择上下文
2. **缓存机制**：对相似的匹配请求进行缓存，减少 API 调用
3. **批量匹配**：将多个文本块合并为一次 API 调用
4. **置信度评分**：返回匹配的置信度分数，供后续流程使用

